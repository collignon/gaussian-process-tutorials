{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The plot\n",
    "\n",
    "- Neural Nets are simple mathematical models defining a function.\n",
    "- A Neural Net with a fixed structure is characterized by the unit biases and connection weights parameters\n",
    "- Thus a prior over biases and weights implies a prior over functions.\n",
    "- However, the meaning of weights and biases in neural nets is obscure.\n",
    "- Moreover, the number of hidden units (in a multilayer perceptron with single hidden layer) limits the set of functions representable by the neural net.\n",
    "- By increasing the number of hidden units to infinity, and by selecting appropriate priors for the neural net parameters, the corresponding prior over functions is a Gaussian Process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background on Neural Nets\n",
    "- [X] Mathematical formulation\n",
    "- [X] Multilayer Perceptron\n",
    "- [X] Representation of a single hidden-layer perceptron as a DAG, with labels for unit biases and connection weights\n",
    "- [X] Mathematical expression for the corresponding perceptron with $\\tanh$ activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks are mathematical models for defining a function mapping inputs $I$ to outputs $O$, $$\\,f: X \\rightarrow Y$\n",
    "\n",
    "A neural network is composed of a number of connected **units**. The way that the units are connected to each other determine the *network structure*. In this tutorial, we will focus on **multilayer perceptrons**. The multilayer perceptron has three main properties:\n",
    "\n",
    "1. *Feedforward connections* - connections between units are unidirectional and there are no cycles.\n",
    "2. *Layered units* - the network is organized in layers, with each unit being connected only to units in the previous and next layers.\n",
    "3. *Multiple layers* - there are more than two layers.\n",
    "\n",
    "![alt text](multilayer_perceptron.png \"Title\")\n",
    "\n",
    "A multilayer perceptron with $I$ **input** and $O$ **output** units takes in a set of real inputs, $\\mathbf{x} := \\{x_i\\}_{i=1}^I$, and computes the real outputs, $\\mathbf{y}:=\\{f_k(\\mathbf{x})\\}_{k=1}^O$, using one or more layers of **hidden** units. In a network with one hidden layer, as in the figure above, the computations can be summarized as \n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "    f_k(\\mathbf{x}) &= b_k + \\sum_j v_{jk} h_j(\\mathbf{x})\n",
    "    \\\\\n",
    "    h_j(\\mathbf{x}) &= K\\left(a_j + \\sum_i u_{ij}x_i\\right)\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "Here, $u_{ij}$ is the connection weight from the input unit $i$ to the hidden unit $j$, and $v_{jk}$ is the connection weight from hidden unit $j$ to output unit $k$. Each output unit $k$ and hidden unit $j$ is associated with a *unit bias*, $b_k$ and $a_j$, respectively. Each hidden unit passes its input values through an **activation function**, $K$, which is usually a nonlinear function, such as the [hyperbolic tangent](https://en.wikipedia.org/wiki/Hyperbolic_function#Standard_analytic_expressions). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting priors on Neurals Nets\n",
    "- Everything is zero-mean Gaussian\n",
    "- The number of hidden units increases to infinity\n",
    "- Prove that the the joint distribution of the values of the function at any finite number of points is multivariate Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph for generating smooth functions with Neural Nets\n",
    "*Make it interactive, let the user pick the number of hidden units\n",
    "\n",
    "## Graph for generating smooth functions with Gaussian Processes\n",
    "*RBF kernel, similar to the GP_tutorial_one\n",
    "\n",
    "## Show the relationship between GP and Neural Net covariance for smooth functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra\n",
    "\n",
    "- Sample Brownian motion functions from a GP and from a NN (with increasing number of hidden units)\n",
    "- Show relationship between covariance functions\n",
    "\n",
    "# Possible extra\n",
    "\n",
    "- Talk about priors for networks with more than one hidden layers?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (whatever you want to call it)",
   "language": "python",
   "name": "envname"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
