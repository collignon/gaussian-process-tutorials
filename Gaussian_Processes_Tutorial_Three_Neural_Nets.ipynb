{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The plot\n",
    "\n",
    "- Neural Nets are simple mathematical models defining a function.\n",
    "- A Neural Net with a fixed structure is characterized by the unit biases and connection weights parameters\n",
    "- Thus a prior over biases and weights implies a prior over functions.\n",
    "- However, the meaning of weights and biases in neural nets is obscure.\n",
    "- Moreover, the number of hidden units (in a multilayer perceptron with single hidden layer) limits the set of functions representable by the neural net.\n",
    "- By increasing the number of hidden units to infinity, and by selecting appropriate priors for the neural net parameters, the corresponding prior over functions is a Gaussian Process.\n",
    "\n",
    "This tutorial has two objectives:\n",
    "\n",
    "1. To show how neural networks can be used for building priors over spaces of functions\n",
    "2. To show that there is a correspondence between the neural networks and gaussian processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background on Neural Nets\n",
    "- [X] Mathematical formulation\n",
    "- [X] Multilayer Perceptron\n",
    "- [X] Representation of a single hidden-layer perceptron as a DAG, with labels for unit biases and connection weights\n",
    "- [X] Mathematical expression for the corresponding perceptron with $\\tanh$ activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks are mathematical models for defining a function mapping inputs $X$ to outputs $Y$, \n",
    "\n",
    "$$\\,f: X \\rightarrow Y$$\n",
    "\n",
    "A neural network is composed of a number of connected **units**. The way that the units are connected to each other determines the *network structure*. In this tutorial, we will focus on **multilayer perceptrons**. The multilayer perceptron has three main properties:\n",
    "\n",
    "1. *Feedforward connections* - connections between units are unidirectional and there are no cycles.\n",
    "2. *Layered units* - the network is organized in layers, with each unit being connected only to units in the previous and next layers.\n",
    "3. *Multiple layers* - there are more than two layers.\n",
    "\n",
    "![Example Multilayer Perceptron](multilayer_perceptron.png \"Example Multilayer Perceptron with a single hidden layer\")\n",
    "\n",
    "A multilayer perceptron with $I$ **input** and $O$ **output** units takes in a set of real inputs, $\\mathbf{x} := \\{x_i\\}_{i=1}^I$, and computes the real outputs, $\\mathbf{y}:=\\{f_k(\\mathbf{x})\\}_{k=1}^O$, using one or more layers of **hidden** units. In a network with one hidden layer, as in the figure above, the computations can be summarized as \n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "    f_k(\\mathbf{x}) &= b_k + \\sum_j v_{jk} h_j(\\mathbf{x})\n",
    "    \\\\\n",
    "    h_j(\\mathbf{x}) &= K\\left(a_j + \\sum_i u_{ij}x_i\\right)\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "Here, $u_{ij}$ is the connection weight from the input unit $i$ to the hidden unit $j$, and $v_{jk}$ is the connection weight from hidden unit $j$ to output unit $k$. Each output unit $k$ and hidden unit $j$ is associated with a *unit bias*, $b_k$ and $a_j$, respectively. Each hidden unit passes its input values through an **activation function**, $K$, which is usually a nonlinear function, such as the [hyperbolic tangent](https://en.wikipedia.org/wiki/Hyperbolic_function#Standard_analytic_expressions). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting priors on Neurals Nets\n",
    "- Everything is zero-mean Gaussian\n",
    "- The number of hidden units increases to infinity\n",
    "- Prove that the the joint distribution of the values of the function at any finite number of points is multivariate Gaussian\n",
    "\n",
    "We want to use neural networks to induce priors over spaces of functions. \n",
    "\n",
    "In a multilayer perceptron with a single hidden layer and fixed number of units in each layer, the remaining parameters are the connections weights, $u_{ij}$ and $v_{jk}$, and unit biases, $a_j$ and $b_k$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stating the assumptions \n",
    "- The activation function is bounded\n",
    "- $b_k$ and $v_{jk}$ are independent and normally distributed with mean zero and fixed finite variance\n",
    "- $v_{jk}$ has mean zero and \n",
    "- $a_j$ and $u_{ij}$ are independent and identically distributed (separately)\n",
    "- Only one input and one output units\n",
    "\n",
    "### Remembering the central limit theorem\n",
    "\n",
    "Let $\\{X_1, X_2, \\dots, X_n\\}$ be a sequence of independent random variables, in which each $X_i$ has finite mean and variance, $\\mathbb{E}[X_i] = \\mu_i$ and $\\text{Var}[X_i] = \\sigma_i^2$.\n",
    "\n",
    "### The expectation of each hidden unit's contribution to the output is zero\n",
    "\n",
    "### The variance of each hidden unit's contribution to the output is finite (if $\\sigma_v$ decreases with H)\n",
    "\n",
    "### The output tends to Gaussian through the CLT\n",
    "\n",
    "### Similarly several inputs leading to several outputs tends to a multivariate Gaussian -> Gaussian Process\n",
    "\n",
    "### Changing the activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph for generating smooth functions with Neural Nets\n",
    "*Make it interactive, let the user pick the number of hidden units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilayerPerceptron():\n",
    "    '''Fully Connected Multilayer FeedForward Artificial Neural Network.'''\n",
    "    def __init__(self, num_layers, num_input, num_output, num_hidden, \n",
    "                 activation_function,):\n",
    "        pass\n",
    "    \n",
    "    class Neuron():\n",
    "        '''Computational unit of a Neural Network.'''\n",
    "        def __init__(self, bias, input_weights, input_values, activation_function):\n",
    "            if len(input_weights) != len(input_values):\n",
    "                raise ValueError('Different number of input values and weights.')\n",
    "            pass\n",
    "        \n",
    "        def compute_output(x):\n",
    "            '''Compute linear combination of input_values, weighted by input_weights'''\n",
    "            pass\n",
    "    \n",
    "    \n",
    "class BayesianPerceptron():\n",
    "    def __init__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test Multilayer Perceptron\n",
    "\n",
    "## A NN with only one neuron outputs the same value as the input\n",
    "nn = MultilayerPerceptron()\n",
    "assert "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "identity = lambda x: x\n",
    "identity(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph for generating smooth functions with Gaussian Processes\n",
    "*RBF kernel, similar to the GP_tutorial_one\n",
    "\n",
    "## Show the relationship between GP and Neural Net covariance for smooth functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra\n",
    "\n",
    "- Sample Brownian motion functions from a GP and from a NN (with increasing number of hidden units)\n",
    "- Show relationship between covariance functions\n",
    "\n",
    "# Possible extra\n",
    "\n",
    "- Talk about priors for networks with more than one hidden layers?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (whatever you want to call it)",
   "language": "python",
   "name": "envname"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
